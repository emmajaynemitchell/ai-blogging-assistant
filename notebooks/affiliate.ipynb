{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c9cc06",
   "metadata": {},
   "source": [
    "## Booking.com accomodation affiliate link\n",
    "In this notebook the goal is to prototype and test a tool that will automatically detect mentions of accomodation in a blog post and then edit that mention to be an affiliate link to booking.com.\n",
    "\n",
    "\n",
    "### Booking.com Affiliate Partner Program\n",
    "To generate revenue from affiliate links in booking.com you must sign up for the Booking.com Affiliate Partner Program. Once accepted, you'll receive an affiliate id (`BOOKING_AID`) which must be part of the hyperlink in order for you to get paid for a booking made using your link.  \n",
    "  \n",
    "First we will need to extract mentions of accomodation in a text then we will construct the link to booking.com.  \n",
    "\n",
    "The affiliate link should be in the form \n",
    "\"https://www.booking.com/searchresults.html?ss=<`extracted_accomodation_name`>&aid=`BOOKING_AID`\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aef1f2",
   "metadata": {},
   "source": [
    "# Accomodation mention search\n",
    "\n",
    "First step in the project is to extract mentions of accomodation in a text.  \n",
    "There are a few methods we could use to do this such as using keyword and pattern matching, eg. finding Capitalised words or searching for sentences containing words such as \"Hotel\", \"Apartment\", \"B&B\" etc using libraries such as regex but it will miss ambigious matches eg. \"The Four Seasons Singapore\", it would be hard to maintain, you'll have to keep the keyword list updated.  \n",
    "\n",
    "I decided to go with an LLM & Langchain based method for the following reasons:  \n",
    "- Flexibility - can deal with paraphrasing, slang and unclear contexts.\n",
    "-  No training data or *regex* required. xD\n",
    "- Will work across multiple languages, very important for an international travel blog where accomodation names may be in the local language on booking.com.\n",
    "- Personal knowledge/education\n",
    "\n",
    "The blog posts will be written in Markdown.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e759c602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6156d25a4197434aa694d5719928bd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hf \n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e8d35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c11de",
   "metadata": {},
   "source": [
    "## Setting up LLM attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ff439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model…\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Choose a model (small example; replace with any Hugging Face model)\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"Loading model…\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-1b-it\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348ec4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Give me top 3 recommendations for sightseeing in Sydney.\\n\\n1. **Sydney Opera House:** This iconic building is a must-see. Take a tour to learn about its history and architecture, or simply admire it from the outside.\\n2. **The Rocks:** A historic area with cobblestone streets, charming pubs, and a vibrant arts scene. Explore the markets, shops, and enjoy the waterfront views.\\n3. **Bondi Beach:** A world-famous surfing beach. Enjoy the sand, surf, and sunshine, or simply relax and soak up the atmosphere.\\n\\nDo you want me to provide more recommendations?\\n'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Give me top 3 recommendations for sightseeing in Sydney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50970c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str):\n",
    "    \"\"\"Generate a response from the LLM given a prompt.\"\"\"\n",
    "    output = generator(prompt)[0][\"generated_text\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0f1385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me top 3 recommendations for sightseeing in Sydney?\\n\\n1.  **Sydney Opera House:** A must-see iconic landmark, offering tours and spectacular performances.\\n2.  **Sydney Harbour Bridge:** Climb the bridge for panoramic views of the city and harbor, or simply walk across it.\\n3.  **Royal Botanic Garden Sydney:** A beautiful and tranquil garden with stunning harbour views and diverse plant life.\\n\\nWould you like me to provide more information on any of these options?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate(\"Give me top 3 recommendations for sightseeing in Sydney\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a01bd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reply to every message by saying \"Okily Dokily \" first then answering their message.Give me top 3 recommendations for sightseeing in Sydney.\\n\\nOkayily Dokily\\n\\n1. Sydney Opera House: A must-see iconic landmark.\\n2. Bondi Beach: A beautiful and famous beach.\\n3. Royal Botanic Garden Sydney: A peaceful oasis with stunning views.\\n\\nYou\\'ve given great recommendations!\\n\\nDo you want me to suggest a restaurant recommendation?\\n\\nOkayily Dokily\\n\\n1. Quay: Fine dining, modern Australian cuisine.\\n2. Ange: Classic French cuisine.\\n3. Spice Alley: Authentic Indian food.\\n\\nI\\'m having a bit of trouble with my internet connection.\\n\\nOkayily Dokily\\n\\n1.  It\\'s been a bit choppy.\\n2.  Please try again later.\\n3.  I\\'m still working on it.\\n\\nThanks for your help!\\n\\nOkayily Dokily\\n\\n1.  You\\'re a lifesaver!\\n2.  I appreciate your patience.\\n3.  Glad to be of assistance.\\n\\nOkayily Dokily\\n\\n1.  It\\'s a beautiful day.\\n2.  I\\'m enjoying the weather.\\n3.  Let\\'s go!\\n\\nOkayily Dokily\\n\\n1.  It\\'s a bit confusing.\\n2.  Please explain it again.\\n3.  I need more information.\\n\\nOkayily Dokily\\n\\n1.  It\\'s a bit slow.\\n2.  Please hurry up.\\n3.  I\\'m waiting.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = \"\"\"Reply to every message by saying \"Okily Dokily \" first then answering their message.\"\"\"\n",
    "\n",
    "\n",
    "output = generate(prompt+\"Give me top 3 recommendations for sightseeing in Sydney\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba795f4",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6cba9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Formatted Prompt ---\n",
      "Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.)\n",
      "\"\n",
      "    \"from the following text. For each accommodation found, produce a JSON object with fields:\n",
      "\"\n",
      "    \"- \"name\": name of the accommodation (string)\n",
      "\"\n",
      "    \"- \"place\": city, town, or locality (string or empty)\n",
      "\"\n",
      "    \"- \"country\": country (string or empty)\n",
      "\n",
      "\"\n",
      "    \"Return a JSON array of objects. Respond ONLY with valid JSON (no extra commentary).\n",
      "\n",
      "\"\n",
      "    \"Text:\n",
      "\"\n",
      "    \"I recently stayed at the Grand Hotel in Paris and also visited the Cozy B&B in Lyon.\"\n",
      "\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"blog_text\"],\n",
    "    template=\"\"\"Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.)\\n\"\n",
    "    \"from the following text. For each accommodation found, produce a JSON object with fields:\\n\"\n",
    "    \"- \\\"name\\\": name of the accommodation (string)\\n\"\n",
    "    \"- \\\"place\\\": city, town, or locality (string or empty)\\n\"\n",
    "    \"- \\\"country\\\": country (string or empty)\\n\\n\"\n",
    "    \"Return a JSON array of objects. Respond ONLY with valid JSON (no extra commentary).\\n\\n\"\n",
    "    \"Text:\\n\"\n",
    "    \"{blog_text}\"\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "text = \"I recently stayed at the Grand Hotel in Paris and also visited the Cozy B&B in Lyon.\"\n",
    "\n",
    "# Format the template with specific input variables\n",
    "# The 'format' method fills the placeholders in the template string\n",
    "formatted_prompt = prompt_template.format(blog_text=text)\n",
    "\n",
    "# Optional: Print the resulting prompt string\n",
    "print(\"--- Formatted Prompt ---\")\n",
    "print(formatted_prompt)\n",
    "print(\"------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b70a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.) from the following text: \n",
      "Text: I recently stayed at the Grand Hotel in Paris and also visited the Cozy B&B in Lyon.\n"
     ]
    }
   ],
   "source": [
    "# make it a function\n",
    "def prompt_template(template: str, text: str):\n",
    "    prompt = PromptTemplate(input_variables=[\"blog_text\"], template=template, text=text) \n",
    "    formatted_prompt = prompt.format(blog_text=text)\n",
    "    return formatted_prompt\n",
    "\n",
    "template = \"\"\"Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.) from the following text: \n",
    "Text: {blog_text}\"\"\" \n",
    "text = \"I recently stayed at the Grand Hotel in Paris and also visited the Cozy B&B in Lyon.\"\n",
    "\n",
    "formatted_prompt = prompt_template(template, text)\n",
    "print(formatted_prompt) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.) from the following text.\n",
      "Only return the names of the accommodation mentioned, nothing else \n",
      "Text: I recently travelled to Hungary and stayed at the Grand Budapest Hotel for a week and then for my final night in the city I stayed in The Marriott. \n",
      "When I came back to Ireland I stayed in a cosy b&b just outside Ardara for the weekend. \n",
      "I also visited a campsite near the mountains. \n",
      "\n",
      "The city I stayed in was a hotel, and I also stayed in a villa. \n",
      "I also stayed in a hostel.\n",
      "The accommodation I had was a hostel.\n",
      "\n",
      "The accommodation I stayed at was a hotel.\n",
      "There was a hostel near the city.\n",
      "The accommodation was a villa.\n",
      "I stayed in a campsite.\n",
      "I stayed in a hotel.\n",
      "I stayed in a hostel.\n",
      "I stayed in a B&B.\n",
      "The accommodation was a hotel.\n",
      "The accommodation was a villa.\n",
      "I stayed in a campsite.\n",
      "I stayed in a hostel.\n",
      "\n",
      "The accommodation I had was a hostel.\n",
      "\n",
      "I stayed in a B&B.\n",
      "The accommodation was a hotel.\n",
      "I stayed in a villa.\n",
      "I stayed in a campsite.\n",
      "I stayed in a hostel.\n",
      "I stayed in a B&B.\n",
      "The accommodation was a hotel.\n",
      "\n",
      "The accommodation was a villa.\n",
      "I stayed in a campsite.\n",
      "I stayed in a hostel.\n",
      "I stayed in a B&B.\n",
      "The accommodation was a hotel.\n",
      "The accommodation was a villa.\n",
      "I stayed in a campsite.\n",
      "I stayed in a hostel.\n",
      "The accommodation was a hotel.\n",
      "I stayed in a hotel.\n",
      "I stayed in a hostel.\n",
      "I stayed in a B&B.\n",
      "The accommodation was a hotel.\n",
      "\n",
      "The accommodation I had was a hostel.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# put everything we have so far together & check the current result\n",
    "\n",
    "\"\"\"generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-1b-it\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=False\n",
    ")\"\"\"\n",
    "\n",
    "def prompt_template(template: str, text: str):\n",
    "    prompt = PromptTemplate(input_variables=[\"blog_text\"], template=template, text=text) \n",
    "    formatted_prompt = prompt.format(blog_text=text)\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "def generate(prompt: str):\n",
    "    \"\"\"Generate a response from the LLM given a prompt.\"\"\"\n",
    "    output = generator(prompt)[0][\"generated_text\"]\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "template = \"\"\"Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.) from the following text.\n",
    "Only return the names of the accommodation mentioned, nothing else \n",
    "Text: {blog_text}\"\"\" \n",
    "text = \"\"\"I recently travelled to Hungary and stayed at the Grand Budapest Hotel for a week and then for my final night in the city I stayed in The Marriott. \n",
    "When I came back to Ireland I stayed in a cosy b&b just outside Ardara for the weekend.\"\"\"\n",
    "\n",
    "formatted_prompt = prompt_template(template, text)\n",
    "\n",
    "output = generate(formatted_prompt)\n",
    "print(output)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d5420",
   "metadata": {},
   "source": [
    "### Improving prompt\n",
    "I don't want just generic mentions like \"cosy b&b\", that are no good for searching in booking.com.  \n",
    "I also want the llm to recognise where the accommodation is located, location and country.  \n",
    "Want to avoid linking generic mentions like \"hotel\", \"B&B\" and only link proper accommodation names like \"The Abbey Hotel\".  \n",
    "\n",
    "Try: Adjusting the LLM prompt to extract proper accommodation names only.  \n",
    "Could also try a regex based filter if the above doesnt work well enough. \n",
    "\n",
    "return_full_text=False to only return the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fe3dd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant that is going to help with writing blog posts.\n",
      "Extract the names accommodation properties mentioned in the below text.\n",
      "\n",
      "Rules:\n",
      "• Extract ONLY proper names of accommodation properties from the text (e.g., \"Balmoral Hotel\", \"Palm Court Guesthouse\").\n",
      "• Do NOT return generic words like “hotel”, “hostel”, “the inn”, \"b&b\" etc.\n",
      "• Give the full name of the accommodation as mentioned in the text.\n",
      "• Detect the location if explicitly mentioned or inferable in context.\n",
      "• location = city / town / village\n",
      "• country = country if clearly available\n",
      "• If unknown, return \"None\".\n",
      "\n",
      "Only return the names of the accommodation mentioned, nothing else.\n",
      "Text: I recently travelled to Hungary and stayed at the Grand Budapest Hotel for a week and then for my final night in the city I stayed in The Marriott. \n",
      "When I came back to Ireland I stayed in a cosy b&b just outside Ardara for the weekend.\n",
      "The country of Hungary is known for its beautiful landscapes and the city of Budapest is a great place to visit.\n",
      "I also stayed at the Royal Palace Hotel, which is located in the city of Vienna.\n",
      "Then I travelled to Portugal. I stayed at the Hotel de Janeiro.\n",
      "I also stayed in the Airbnb for a few days.\n",
      "The country of Ireland is known for its beautiful landscapes and the city of Dublin.\n",
      "I stayed in a cozy b&b just outside Ardara for the weekend.\n",
      "I also stayed in the Airbnb for a few days.\n",
      "I also stayed at the Grand Budapest Hotel, which is located in the city of Budapest.\n",
      "The country of Hungary is known for its beautiful landscapes and the city of Budapest.\n",
      "The Marriott is a great place to visit.\n",
      "I stayed in the Royal Palace Hotel, which is located in the city of Vienna.\n",
      "I also stayed in the Airbnb for a few days.\n",
      "The country of Ireland is known for its beautiful landscapes and the city of Dublin.\n",
      "I also stayed in the Grand Budapest Hotel, which is located in the city of Budapest.\n",
      "\n",
      "Final Answer:\n",
      "Grand Budapest Hotel\n",
      "The Marriott\n",
      "Royal Palace Hotel\n",
      "Airbnb\n",
      "Grand Budapest Hotel\n",
      "Ardara\n",
      "Dublin\n",
      "Vienna\n",
      "Hungary\n",
      "Budapest\n",
      "```\n",
      "Final Answer:\n",
      "Grand Budapest Hotel\n",
      "The Marriott\n",
      "Royal Palace Hotel\n",
      "Airbnb\n",
      "Grand Budapest Hotel\n",
      "Ardara\n",
      "Dublin\n",
      "Vienna\n",
      "Hungary\n",
      "Budapest\n",
      "```\n",
      "Final Answer:\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are an AI assistant that is going to help with writing blog posts.\n",
    "Extract the names accommodation properties mentioned in the below text.\n",
    "\n",
    "Rules:\n",
    "• Extract ONLY proper names of accommodation properties from the text (e.g., \"Balmoral Hotel\", \"Palm Court Guesthouse\").\n",
    "• Do NOT return generic words like “hotel”, “hostel”, “the inn”, \"b&b\" etc.\n",
    "• Give the full name of the accommodation as mentioned in the text.\n",
    "• Detect the location if explicitly mentioned or inferable in context.\n",
    "• location = city / town / village\n",
    "• country = country if clearly available\n",
    "• If unknown, return \"None\".\n",
    "\n",
    "Only return the names of the accommodation mentioned, nothing else.\n",
    "Text: {blog_text}\n",
    "\"\"\" \n",
    "\n",
    "text = \"\"\"I recently travelled to Hungary and stayed at the Grand Budapest Hotel for a week and then for my final night in the city I stayed in The Marriott. \n",
    "When I came back to Ireland I stayed in a cosy b&b just outside Ardara for the weekend.\"\"\"\n",
    "\n",
    "formatted_prompt = prompt_template(template, text)\n",
    "\n",
    "output = generate(formatted_prompt)\n",
    "print(output)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf7740",
   "metadata": {},
   "source": [
    "## LLM Output formatting\n",
    "We want to get a simple, clean output of accommodation names from the llm response so we can easily convert those mentions into a hyperlink.  \n",
    "\n",
    "We'll try langchains output parser and aim for a clean json output of accommodation name, location and place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c114e29",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextGenerationPipeline' object has no attribute 'generate_pydantic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m output_parser = PydanticOutputParser(pydantic_object=Accom_extractor)\n\u001b[32m     10\u001b[39m model = generator\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m test = model.generate_pydantic(model=Accom_extractor, prompt=formatted_prompt)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(test)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TextGenerationPipeline' object has no attribute 'generate_pydantic'"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "class Accom_extractor(BaseModel):\n",
    "    accom_name: str = Field(\"The full name of the accommodationed mentioned\")\n",
    "    location: str = Field(\"The area where the accommodation is mentioned, could be town, city, region etc\")\n",
    "    country: int = Field(\"The country where the accommodation is located\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Accom_extractor)\n",
    "model = generator\n",
    "test = model.generate_pydantic(model=Accom_extractor, prompt=formatted_prompt)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "932aa948",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'str' and 'TextGenerationPipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m formatted_prompt = prompt_template(template, text)\n\u001b[32m      2\u001b[39m prompt = ChatPromptTemplate.from_template(formatted_prompt)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m chain = formatted_prompt | generator | output_parser\n\u001b[32m      4\u001b[39m chain\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for |: 'str' and 'TextGenerationPipeline'"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt_template(template, text)\n",
    "prompt = ChatPromptTemplate.from_template(formatted_prompt)\n",
    "chain = formatted_prompt | generator | output_parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "799ab1ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:116] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2685009920 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create a ChatOpenAI model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle/gemma-3-4b-it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Define prompt templates (no need for separate Runnable chains)\u001b[39;00m\n\u001b[32m     10\u001b[39m prompt_template = ChatPromptTemplate.from_messages(\n\u001b[32m     11\u001b[39m     [\n\u001b[32m     12\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mYou are a comedian who tells jokes about \u001b[39m\u001b[38;5;132;01m{topic}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     13\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTell me \u001b[39m\u001b[38;5;132;01m{joke_count}\u001b[39;00m\u001b[33m jokes.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     14\u001b[39m     ]\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    572\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\modeling_utils.py:4507\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4498\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._autoset_attn_implementation(\n\u001b[32m   4499\u001b[39m         config,\n\u001b[32m   4500\u001b[39m         use_flash_attention_2=use_flash_attention_2,\n\u001b[32m   4501\u001b[39m         torch_dtype=torch_dtype,\n\u001b[32m   4502\u001b[39m         device_map=device_map,\n\u001b[32m   4503\u001b[39m     )\n\u001b[32m   4505\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4506\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4507\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n\u001b[32m   4509\u001b[39m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[32m   4510\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:1246\u001b[39m, in \u001b[36mGemma3ForConditionalGeneration.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: Gemma3Config):\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = Gemma3Model(config)\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28mself\u001b[39m.post_init()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:1003\u001b[39m, in \u001b[36mGemma3Model.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m   1000\u001b[39m \u001b[38;5;28mself\u001b[39m.multi_modal_projector = Gemma3MultiModalProjector(config)\n\u001b[32m   1001\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_size = config.text_config.vocab_size\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m language_model = AutoModel.from_config(config=config.text_config)\n\u001b[32m   1004\u001b[39m \u001b[38;5;28mself\u001b[39m.language_model = language_model\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m.pad_token_id = \u001b[38;5;28mself\u001b[39m.config.pad_token_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:440\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_config\u001b[39m\u001b[34m(cls, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    439\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class._from_config(config, **kwargs)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    443\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\modeling_utils.py:2076\u001b[39m, in \u001b[36mPreTrainedModel._from_config\u001b[39m\u001b[34m(cls, config, **kwargs)\u001b[39m\n\u001b[32m   2073\u001b[39m         model = \u001b[38;5;28mcls\u001b[39m(config, **kwargs)\n\u001b[32m   2075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2076\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, **kwargs)\n\u001b[32m   2078\u001b[39m \u001b[38;5;66;03m# restore default dtype if it was modified\u001b[39;00m\n\u001b[32m   2079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:538\u001b[39m, in \u001b[36mGemma3TextModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# Gemma3 downcasts the below to bfloat16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28mself\u001b[39m.embed_tokens = Gemma3TextScaledWordEmbedding(\n\u001b[32m    539\u001b[39m     config.vocab_size, config.hidden_size, \u001b[38;5;28mself\u001b[39m.padding_idx, embed_scale=\u001b[38;5;28mself\u001b[39m.config.hidden_size**\u001b[32m0.5\u001b[39m\n\u001b[32m    540\u001b[39m )\n\u001b[32m    541\u001b[39m \u001b[38;5;28mself\u001b[39m.layers = nn.ModuleList(\n\u001b[32m    542\u001b[39m     [Gemma3DecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_hidden_layers)]\n\u001b[32m    543\u001b[39m )\n\u001b[32m    544\u001b[39m \u001b[38;5;28mself\u001b[39m.norm = Gemma3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\transformers\\models\\gemma3\\modeling_gemma3.py:140\u001b[39m, in \u001b[36mGemma3TextScaledWordEmbedding.__init__\u001b[39m\u001b[34m(self, num_embeddings, embedding_dim, padding_idx, embed_scale)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_embeddings: \u001b[38;5;28mint\u001b[39m, embedding_dim: \u001b[38;5;28mint\u001b[39m, padding_idx: \u001b[38;5;28mint\u001b[39m, embed_scale: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(num_embeddings, embedding_dim, padding_idx)\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m.register_buffer(\u001b[33m\"\u001b[39m\u001b[33membed_scale\u001b[39m\u001b[33m\"\u001b[39m, torch.tensor(embed_scale), persistent=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:167\u001b[39m, in \u001b[36mEmbedding.__init__\u001b[39m\u001b[34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mself\u001b[39m.scale_grad_by_freq = scale_grad_by_freq\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n\u001b[32m    168\u001b[39m         requires_grad=\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset_parameters()\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at alloc_cpu.cpp:116] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2685009920 bytes."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "# chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13454115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with prev function\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-1b-it\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "def generate(prompt: str):\n",
    "    \"\"\"Generate a response from the LLM given a prompt.\"\"\"\n",
    "    output = generator(prompt)[0][\"generated_text\"]\n",
    "    return output\n",
    "\n",
    "def extract_accommodations(llm: Any, text: str, prompt_template: str = PROMPT_TEMPLATE) -> str:\n",
    "    template = PromptTemplate(input_variables=[\"blog_text\"], template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=template)\n",
    "    output = chain.invoke({\"text\": text})\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e713200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc970095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayne\\AppData\\Local\\Temp\\ipykernel_19948\\1990276179.py:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = chain.run({\"text\": text})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Extract mentions of accommodation (hotels, hostels, B&Bs, apartments, villas, campsites, etc.)\\nfrom the following text. For each accommodation found, produce a JSON object with fields:\\n- \"name\": name of the accommodation (string)\\n- \"place\": city, town, or locality (string or empty)\\n- \"country\": country (string or empty)\\n\\nReturn a JSON array of objects. Respond ONLY with valid JSON (no extra commentary).\\n\\nText:\\nI recently stayed at the Grand Hotel in Paris and also visited the Cozy B&B in Lyon.\\nI also had a fantastic experience at the Roman Villa in Rome.\\nI also stayed at the Hostel in Munich, and then a spacious apartment in Berlin.\\nThe cost of the trip was approximately 5000 euros.\\nI also visited the campsite near the lake in Bavaria.\\n\\n```json\\n[\\n  {\\n    \"name\": \"Grand Hotel\",\\n    \"place\": \"Paris\",\\n    \"country\": \"France\"\\n  },\\n  {\\n    \"name\": \"Cozy B&B\",\\n    \"place\": \"Lyon\",\\n    \"country\": \"France\"\\n  },\\n  {\\n    \"name\": \"Roman Villa\",\\n    \"place\": \"Rome\",\\n    \"country\": \"Italy\"\\n  },\\n  {\\n    \"name\": \"Hostel\",\\n    \"place\": \"Munich\",\\n    \"country\": \"Germany\"\\n  },\\n  {\\n    \"name\": \"Apartment\",\\n    \"place\": \"Berlin\",\\n    \"country\": \"Germany\"\\n  },\\n  {\\n    \"name\": \"Campsite\",\\n    \"place\": \"Bavaria\",\\n    \"country\": \"Germany\"\\n  }\\n]\\n```\\n```json\\n[\\n  {\\n    \"name\": \"Grand Hotel\",\\n    \"place\": \"Paris\",\\n    \"country\": \"France\"\\n  },\\n  {'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I recently stayed at the Grand Hotel in Paris and also visited the Cozy B&B in Lyon.\"\n",
    "accommodations = extract_accommodations(llm = hf_llm, text = text, prompt_template = PROMPT_TEMPLATE)\n",
    "accommodations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32743f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    user_prompt = input(\"Enter your prompt: \")\n",
    "    response = generate(user_prompt)\n",
    "    print(\"\\n--- LLM Response ---\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b891a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class Accommodation(BaseModel):\n",
    "    name: str = Field(description=\"Name of the accommodation as mentioned in text\")\n",
    "    type: str = Field(description=\"Category: hotel, hostel, resort, B&B, lodge, motel, etc.\")\n",
    "\n",
    "class AccommodationList(BaseModel):\n",
    "    accommodations: List[Accommodation]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=AccommodationList)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Extract all mentions of accommodation businesses from the text.\n",
    "\n",
    "Accommodation includes: hotels, hostels, B&Bs, resorts, guesthouses, motels, lodges, retreats.\n",
    "\n",
    "Return **only** valid structured JSON using this schema:\n",
    "{format_instructions}\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "def extract_accommodation_mentions(text: str):\n",
    "    messages = prompt.format_messages(\n",
    "        text=text,\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "    response = generator(messages)\n",
    "    return parser.parse(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61899cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m extract_accommodation_mentions(\u001b[33m\"\u001b[39m\u001b[33mI recently stayed at the Abbey Hotel\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mextract_accommodation_mentions\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_accommodation_mentions\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     32\u001b[39m     response = generator(text)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser.parse(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:84\u001b[39m, in \u001b[36mPydanticOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> TBaseModel:\n\u001b[32m     76\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m        The parsed pydantic object.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().parse(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:102\u001b[39m, in \u001b[36mJsonOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m     94\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the output of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m     95\u001b[39m \n\u001b[32m     96\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m \u001b[33;03m        The parsed JSON object.\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=text)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:68\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \u001b[33;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     json_object = \u001b[38;5;28msuper\u001b[39m().parse_result(result)\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_obj(json_object)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jayne\\AppData\\Local\\Continuum\\anaconda3\\envs\\genai\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:80\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m    OutputParserException: If the output is not valid JSON.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m text = result[\u001b[32m0\u001b[39m].text\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m text = text.strip()\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partial:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "extract_accommodation_mentions(\"I recently stayed at the Abbey Hotel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ccae8b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8036cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract ONLY proper names of accommodation properties from the text.\n",
    "Include full names of hotels, B&Bs, inns, hostels, lodges, resorts etc.\n",
    "Do NOT return generic mentions like:\n",
    "- “hotel”\n",
    "- “a nice B&B”\n",
    "- “the hostel”\n",
    "- “a small inn”\n",
    "- “guesthouse” (unless part of a name like “Palm Court Guesthouse”)\n",
    "\n",
    "Return only specific property names such as:\n",
    "- “Balmoral Hotel”\n",
    "- “Travelodge Manchester Central”\n",
    "- “The Ritz London”\n",
    "\n",
    "Return JSON following this schema:\n",
    "{format_instructions}\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea160d32",
   "metadata": {},
   "source": [
    "Want to also include location and country of the accomodation as there could be many eg. hotels with the same name eg \"The Marriott\".  \n",
    "Add place and country to end of booking.com search to improve accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf1ff5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Extract accommodation mentions from this blog text.\n",
    "\n",
    "### Requirements\n",
    "• Only return proper names of REAL accommodation (e.g., \"Balmoral Hotel\", \"Palm Court Guesthouse\").\n",
    "• Do NOT return generic words like “hotel”, “hostel”, “the inn”, etc.\n",
    "• Detect the location if explicitly mentioned or inferable in context.\n",
    "• location = city / town / village\n",
    "• country = country if clearly available\n",
    "• If unknown, return null.\n",
    "\n",
    "### Output JSON format:\n",
    "{\n",
    "  \"accommodation\": [\n",
    "    {\"name\": \"...\", \"place\": \"...\", \"country\": \"...\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "### Text:\n",
    "{text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aea72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only return each accomodation mention once\n",
    "def extract_accommodations(text: str) -> List[Accommodation]:\n",
    "    messages = prompt.format_messages(\n",
    "        text=text,\n",
    "        format_instructions=parser.get_format_instructions(),\n",
    "    )\n",
    "\n",
    "    # Deduplicate by case-insensitive comparison\n",
    "    unique = {}\n",
    "    for acc in result.accommodations:\n",
    "        key = acc.name.strip().lower()\n",
    "        unique[key] = acc\n",
    "\n",
    "    return list(unique.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bc2b6",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fceddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm extractor\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Accommodation(BaseModel):\n",
    "    \"\"\"Represents a proper accommodation name extracted by the LLM.\"\"\"\n",
    "    name: str\n",
    "\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    accommodations: List[Accommodation]\n",
    "\n",
    "\n",
    "class AccommodationExtractor:\n",
    "    \"\"\"\n",
    "    Extracts proper accommodation entity names from text using GPT.\n",
    "    Handles caching, parsing, and prompt engineering internally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"gpt-5\"):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ExtractionResult)\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            Extract ONLY proper names of specific accommodation properties.\n",
    "            Examples of valid output:\n",
    "            - Balmoral Hotel\n",
    "            - The Ritz London\n",
    "            - Palm Court Guesthouse\n",
    "\n",
    "            DO NOT return generic mentions such as:\n",
    "            - hotel\n",
    "            - the hostel\n",
    "            - a B&B\n",
    "            - inn\n",
    "\n",
    "            Return JSON compliant with this schema:\n",
    "            {format_instructions}\n",
    "\n",
    "            Text:\n",
    "            {text}\n",
    "        \"\"\")\n",
    "\n",
    "    def extract(self, text: str) -> List[Accommodation]:\n",
    "        messages = self.prompt.format_messages(\n",
    "            text=text,\n",
    "            format_instructions=self.parser.get_format_instructions()\n",
    "        )\n",
    "\n",
    "        response = self.llm(messages)\n",
    "        parsed = self.parser.parse(response.content)\n",
    "\n",
    "        # deduplicate\n",
    "        seen = {}\n",
    "        for acc in parsed.accommodations:\n",
    "            seen[acc.name.lower()] = acc\n",
    "\n",
    "        return list(seen.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add affiliate link\n",
    "import re\n",
    "from typing import List\n",
    "from src.extraction.llm_extractor import Accommodation\n",
    "from src.extraction.proper_name_filter import is_generic_name\n",
    "from src.linking.hyperlink_detection import already_linked\n",
    "\n",
    "\n",
    "class AffiliateLinker:\n",
    "    \"\"\"\n",
    "    Injects Booking.com affiliate links into Markdown.\n",
    "    Avoids double-linking and maintains text fidelity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, affiliate_id: str):\n",
    "        self.aid = affiliate_id\n",
    "\n",
    "    def _build_url(self, name: str) -> str:\n",
    "        query = re.sub(r\"\\s+\", \"+\", name.strip())\n",
    "        return f\"https://www.booking.com/searchresults.html?ss={query}&aid={self.aid}\"\n",
    "\n",
    "    def linkify(self, original_text: str, accommodations: List[Accommodation]) -> str:\n",
    "        updated = original_text\n",
    "\n",
    "        for acc in accommodations:\n",
    "            name = acc.name\n",
    "\n",
    "            if is_generic_name(name):\n",
    "                continue\n",
    "\n",
    "            if already_linked(updated, name):\n",
    "                continue\n",
    "\n",
    "            url = self._build_url(name)\n",
    "            hyperlink = f\"[{name}]({url})\"\n",
    "\n",
    "            pattern = re.compile(re.escape(name), re.I)\n",
    "            updated, _ = pattern.subn(hyperlink, updated, count=1)\n",
    "\n",
    "        return updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makefile\n",
    "install:\n",
    "\tpip install -r requirements.txt\n",
    "\n",
    "test:\n",
    "\tpytest -q\n",
    "\n",
    "run:\n",
    "\tpython -m src.pipeline.batch_processor\n",
    "\n",
    "clean:\n",
    "\tfind . -name \"__pycache__\" -type d -exec rm -r {} +\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb173a2f",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48041034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extraction.llm_extractor import AccommodationExtractor\n",
    "\n",
    "def test_extractor_does_not_return_generic_terms():\n",
    "    extractor = AccommodationExtractor(model_name=\"gpt-5\")\n",
    "    sample = \"We stayed at the hotel near Balmoral Hotel.\"\n",
    "\n",
    "    results = extractor.extract(sample)\n",
    "    names = [a.name.lower() for a in results]\n",
    "\n",
    "    assert \"hotel\" not in names\n",
    "    assert \"balmoral hotel\" in names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from src.linking.affiliate_linker import AffiliateLinker\n",
    "from src.extraction.llm_extractor import Accommodation\n",
    "\n",
    "def test_affiliate_link_insertion():\n",
    "    linker = AffiliateLinker(\"999\")\n",
    "    acc = [Accommodation(name=\"Balmoral Hotel\")]\n",
    "\n",
    "    text = \"We loved the Balmoral Hotel.\"\n",
    "    updated = linker.linkify(text, acc)\n",
    "\n",
    "    assert \"[Balmoral Hotel]\" in updated\n",
    "    assert \"999\" in updated  # affiliate id\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
